# 样本不均衡

正样本和负样本的比例相差过大，这种情况在金融科技领域相当常见。需要一个分类器，既能够充分学习到正样本，同时也不会影响到负样本的学习。

这里的方案包括：

> 1. 代价敏感加权方案
> 2. 插值过采样方案
> 3. 半监督算法

# 代价敏感加权方案

要解决样本不平衡的问题，很好的一个办法就是「下探」。「下探」是指在拒绝域当中随机接收一部分样本，损失一部分收益，积累一部分的负样本，供后面模型的学习。

不过下探的缺点很明显，风险越高，成本越高，会造成信用质量的恶化，不是每个平台都愿意承担这部分坏账，并且下探的量应该是多少，这个没有一个合适的参考值。

代价敏感加权方案，就是对**不同的分类的样本**分错以后**赋予不同的权重**更新到损失函数当中，这个时候算法的目标就不是使得分类的误差最小化，而是『使得总体的代价最小化』。

## 代价敏感算法的分类

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcxtxsx459j311e08eacs.jpg)

- 直接法

  - 直接设计能够对不同数量的分类样本进行赋予不同权重的算法模型

- 封装法

  不改变算法本身，而是将非代价敏感型转换成代价敏感型的处理

  - 阈值法
  - 抽样法
    - 权重法
    - costing



比如LightGBM本身就有一个参数可以针对正样本和负样本的比例设置一个不同的权重。

# 采样方案



## 插值过采样方案

在少数样本中进行插值，人工合成新的负样本，也就是「Synthetic Minority Oversampling Technique」,简称SMOTE方法。

### BorderLine SMOTE

#### BOS1

原理是，

> 找到容易错分的分类点$x_i$的所有集合，根据这部分集合去进行插值生成新的点。相比于一般的Borderline SMOTE方法，多了一步找到容易错分类的点，而判断是否容易误分的点的依据就是看这个正样本的点周围的正负样本比例，如果负样本比例比正样本比例还要高的话，那么就是容易错分的点，要放入Danger集合当中。



具体的算法流程如下：

> 1. 一个数据集有正样本和负样本，共同组成整体数据。我们从正样本中抽取一个数据点$x_i$，计算$x_i$周围$m$个近邻点，以及计算$m$个近邻点当中负样本的个数$m'$，计算负样本的比例$r_i^{'}=\dfrac{m^{'}}{m}$。
> 2. 根据$r_i^{'}$的计算结果分别对$x_i$进行不同的处理，不处理或者放入Danger集合当中
> 3. 对Danger集合当中的每个点$x_i$，它们都是容易被错分的点，在**正样本集合P**当中找出每个点的k近邻点
> 4. 对其中s个点(s≤k)进行插值处理生成新的样本，$x_{new}^{+}=x_i+\rho_{ij}*(\hat x_{ij}-x_i), j=1, 2, ……s$

![数据集](https://tva1.sinaimg.cn/large/00831rSTgy1gcxuyu4yndj30he0j6jsn.jpg)



对$x_i$的三种情况进行判断，要不要放入Danger集合当中（Danger集合就是在决策边界的点，容易被错分）

情况一：正样本$x_i$周围的负样本比例在$[0, 0.5)$的范围内，负样本占比小，可以认为$x_i$是安全点，不用处理

![image-20200318102651477](https://tva1.sinaimg.cn/large/00831rSTgy1gcxvent0nvj30j80naq52.jpg)

情况二：正样本$x_i$周围的负样本比例在$[0.5, 1)$的范围内，负样本占比大，$x_i$是容易被错分的点，要放入Danger集合中

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcxvi50vt8j30uu0kojtq.jpg)

情况三：正样本$x_i$周围的负样本比例等于1，说明$x_i$是噪声点，不用处理

**API接口：**

```python
from imblearn.over_sampling import SMOTE
sm = SMOTE(kind='borderline1', random_state=random_state)
X_train, y_train = sm.fit_sample(X_train, y_train)
```







#### BOS2

**Borderline SMOTE2**和**Borderline SMOTE1**不同之处在于，前者在合成新的样本的时候不是只对$x_i$在正样本数据集$P$当中进行插值合成，而是综合了正样本数据集$P$和负样本数据集$N$两者。

生成新样本数据集的策略为：

> 1. 对Danger数据集当中的正样本$x_i$，分别找到其在正样本数据集P和负样本数据集N当中的k近邻个数据点集合$P^{'}$和$N^{'}$
> 2. 对这两个k近邻个数据点集合$P^{'}$和$N^{'}$，分配$\alpha$和$1-\alpha$的比例分别插值生成数据点，这部分打标签为正样本，其中$\alpha>0.5$，这样衍生出来的数据点会更加偏向于正样本。

![示意图](https://tva1.sinaimg.cn/large/00831rSTgy1gcxvu9w1s1j30rk0dq76d.jpg)



**API接口：**

```python
from imblearn.over_sampling import SMOTE
sm = SMOTE(kind='borderline1', random_state=random_state)
X_train, y_train = sm.fit_sample(X_train, y_train)
```

### ADASYN

**算法流程：**

1. 对于正样本数据集$P$当中的每个数据点，找到其k近邻的点
2. 计算k近邻点中负样本的占比，然后将其归一化



计算公式为：

$$\hat r_i=\dfrac{r_i}{\sum_{i=1}^{|P|}r_i}$$

其中

$$\sum_{i=1}^{|P|}\hat r_i=1$$

根据上面求得的比例，去计算每个正样本点要生成多少个正样本点。

$$g_i=\hat r_i*G$$

其中$G$是需要生成的正样本的总数，这样就可以求得每个样本点要生成多少个正样本点，附近负样本比例越高的样本点说明密度越大，越难区分，需要生成更多的样本点，起到「自适应」的作用。

$$x_{new}^{+}=x_i+\rho *(\hat x_i-x_i)，其中i=1, 2, ……$$

### Borderline Oversampling

假设这里的正样本是少数类，负样本是多数类，也就是$$标签为1的数量<<标签为1的数量$$。



总体原则：都是推动新生成的样本点往负样本扩展，因为决策边界的点可以让模型重点去训练，这样得到的模型的决策边界会更加接近于理想的决策边界。

**算法流程：**

1. 训练一个支持向量机模型，找到为正样本的支持向量$SV^+$
2. 考察其k近邻的点的负样本比例，如果占比高于0.5的话，就要用内插法，新生成的点向负样本方向扩展；否则如果负样本比例低于0.5的话，那么就用外插法，扩展正样本比例，同样也是往负样本方向扩展。

#### 内插法

前面用的方法都是内插法

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcz1jwu6chj316q0a4wgn.jpg)

#### 外插法

外插法的示意图，如下。

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcz1n2fqwoj316m0c0ju2.jpg)







## 降采样

### 随机降采样



**降采样可能存在的缺点：**

丢失一部分有价值的信息，使得分离的超平面发生偏移，比如在svm对正样本进行降采样以后，正样本数量减少，决策平面向正样本方向发生偏移

# 半监督学习方案

所谓半监督学习，就是学习器不依赖外界交互，而是通过无标签的样本来提升学习性能。

半监督学习的3点假设：

1. 平滑假设：特征相似的样本具有相同的标签
2. 聚类假设：同一个聚类下的样本具有相同的标签
3. 流形假设：同一流形结构下的样本具有相同的标签

实践结果比较好的半监督学习模型：半监督支持向量机和标签传播算法。

## S3Vm半监督支持向量机

半监督支持向量机，S3VM(Semi-supervised supporting machine)的原理是，利用有标签的样本去训练一个支持向量机，然后用这个支持向量机去预测未标记的样本，给这部分未标记的样本打上标签，接下来利用这部分有标签加上标记的无标签样本一起训练一个新的向量机，如此重复直到收敛。



![IMG_9800](https://tva1.sinaimg.cn/large/00831rSTgy1gcwr7ulfqnj316i0u0x6p.jpg)



如图所示，+号代表有标签的正样本，-号代表有标签的负样本，绿色的圆圈代表没有标签的样本，原本根据有标签的样本训练出来的超平面就是蓝色虚线所示，在加入无标签的样本以后，重新调整超平面，使得超平面通过数据比较稀疏的区域，调整后的超平面为红色线所示。

### 1. TSVM（Transductive Support machine）

其中S3VM中比较经典的模型就是TSVM，也就是直推式半监督支持向量机。特点是，给样本打标签，然后结合有标签的样本和标记的无标签样本一起训练，得到一个间隔最大化的最优超平面。

具体做法是，TSVM会先根据有标签的样本训练一个SVM，然后用SVM去给没有标记的样本打标签。接下来分别计算每个样本到分类超平面的距离：

$$H_i=wx_ib$$

接下来计算出松弛变量的大小，对于一组数据里面的任意两个标签相反的样本的松弛变量之和大于2，则调转它们的标签，迭代重新训练一个模型，直到最终收敛。

备注：

这里之所以有「任意两个标签相反的样本的松弛变量之和大于2，则调转它们的标签」这一步，这是为了判断出可能打错标签的样本并予以修正。

算法过程：

![image-20200317114522663](https://tva1.sinaimg.cn/large/00831rSTgy1gcws21g1n2j31eh0u07fd.jpg)

## 标签传播算法

LP(Label Propagation)

作用：负样本衍生，作为一种社区发现算法用于识别团伙欺诈。

特点：

1. 基于图

### 原理

这里说明下$l$和$u$分别代表的意义，$l$代表着有标签的样本数量，$u$代表着没有标签的样本数量，一般情况下$l<<u$，也就是无标签的样本数要远高于有标签的样本数。

标签传播算法是基于图而生成的，图上面的每个节点都是一个数据点，数据点与数据点之间通过线连接，称为「关系」，在这里关系就是数据与数据的相似度，越相似，关系越强。边上面有个关键值代表着关系的强弱，就是权重$w_{ij}$代表节点$i$与节点$j$之间的连线的权重。

$w_{ij}$的计算公式为：

$$w_{ij}=exp(-\dfrac{||x_i-x_j||^2}{\sigma})$$

$w_{ij}$的大小与两个数据点之间的特征的距离有关。







标签传播算法的2个关键矩阵：

1. $(l+u)*(l+u)$概率传播矩阵T
2. $(l+u)*C$标签矩阵Y



我们定义一个概率传播矩阵T，其中$T_{ij}$为标签$i$传播到标签$j$的概率，计算公式如下：

$$T_{ij}=P(i\rightarrow j)=\dfrac{w_{ij}}{\sum_{k=1}^{l+u}w_{ik}}$$







![概率传播的示意图](https://tva1.sinaimg.cn/large/00831rSTgy1gcx023e48wj30cw098mxg.jpg)

如上图所示，查看传播图的其中一部分，我们可以看到，其中一个节点为$i$，另外跟它相连的一个节点为$j$，它们之间的权重为$w_{ij}$，而$i$和它周围其他相连的节点也是相同的求法，所有的权重之和作为分母，$w_{ij}$作为分子，可以计算出，节点$i$的标签传播到节点$j$的概率$T_{ij}$，将这个值更新到概率传播矩阵$T$当中的对应位置，如此迭代。

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcx0cmc1rrj30q60jcjtx.jpg)

上面的值都是传播的概率。





根据上面的概率矩阵，我们就可以计算出某一行，也就是某个样本各个类别下的概率分布。

我们再定义一个标签矩阵$(l+u)*C$标签矩阵Y，$C$代表着类别数，比如在下面那个图当中，类别数为3。每个节点在每个类别下的概率等于该标注值乘以对应的权重相加得到，更新标签矩阵Y。



![image-20200317163528452](https://tva1.sinaimg.cn/large/00831rSTgy1gcx0fw1tfaj30ka0g4t9w.jpg)

### 算法描述

input：u个未标记数据和l个标记的数据及其标签
output：u个未标记数据的标签
第一步：初始化，利用权重公式来计算每条边的权重$w_{ij}$，得到数据间的相似度

第二步：根据得到的权重$w_{ij}$计算节点$i$传播到$j$的传播概率，更新到传播概率矩阵$T_{ij}$中

第三步：定义一个(l+u)*C的矩阵，对于未标注的数据里面的数是随机产生的
第四步：每个节点根据传播概率按照权重相加的方式更新到概率分布矩阵当中
第五步：对已标注的数据更新到初始值（也就是1或者0），重复第四步，直到收敛





### 算法实现

在实际操作中，如下图所示，红框中的两个点分别代表着正样本和负样本，橘色的是未标记的样本，通过概率传播算法最终能够把相邻的数据点都打上标签。

![图来自《智能风控》](https://tva1.sinaimg.cn/large/00831rSTgy1gcwyu82exjj31j00u0qv5.jpg)

# 总结

在这里总结了3种方法。

第一种是代价敏感算法，通过给不平衡的样本赋予不同的权重，样本数更少的则相应权重更大，使得模型在训练的时候也会更加关注少量样本的预测表现；

第二种是「插值生成」算法， 本质上是利用了少量的负样本的信息去合成一些不存在的负样本，SMOTE对样本和特征都有一定的要求，要先进行样本和特征清洗，再进行过采样。

第三种是「半监督」方法，通过已有的负样本去推测跟负样本表现接近的其他样本的标签，从而实现扩充数据量的目标。泛化能力会更好，但也可能存在给样本打错标签的可能性，精度不能保证。

基于实际场景，首先使用代价敏感加权的方法，然后尝试两种半监督模型去衍生负样本和正样本。

在实际应用中，可以利用TSVM或者LP算法去给负样本进行打标签，增加负样本的数量，然后再将所有的正样本和负样本放入到有监督模型当中进行训练。

# 参考

1. [一起来读西瓜书：第十三章 半监督学习 - 简书](https://www.jianshu.com/p/7d4323c28716)
2. [标签传播算法(Label Propagation Algorithm)_Python_诗蕊的专栏-CSDN博客](https://blog.csdn.net/Katherine_hsr/article/details/82343647)
3. [Github of Albertsr]([https://github.com/Albertsr/Class-Imbalance/tree/master/1.%20Cost%20Sensitive%20Learning](https://github.com/Albertsr/Class-Imbalance/tree/master/1. Cost Sensitive Learning))