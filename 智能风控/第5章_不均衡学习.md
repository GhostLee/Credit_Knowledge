# 样本不均衡

正样本和负样本的比例相差过大，这种情况在金融科技领域相当常见。需要一个分类器，既能够充分学习到正样本，同时也不会影响到负样本的学习。

这里的方案包括：

> 1. 代价敏感加权方案
> 2. 插值过采样方案
> 3. 半监督算法

# 代价敏感加权方案

要解决样本不平衡的问题，很好的一个办法就是「下探」。「下探」是指在拒绝域当中随机接收一部分样本，损失一部分收益，积累一部分的负样本，供后面模型的学习。

不过下探的缺点很明显，风险越高，成本越高，会造成信用质量的恶化，不是每个平台都愿意承担这部分坏账，并且下探的量应该是多少，这个没有一个合适的参考值。



# 插值过采样方案

在少数样本中进行插值，人工合成新的负样本，也就是「Synthetic Minority Oversampling Technique」,简称SMOTE方法。



# 半监督学习方案

所谓半监督学习，就是学习器不依赖外界交互，而是通过无标签的样本来提升学习性能。

半监督学习的3点假设：

1. 平滑假设：特征相似的样本具有相同的标签
2. 聚类假设：同一个聚类下的样本具有相同的标签
3. 流形假设：同一流形结构下的样本具有相同的标签

实践结果比较好的半监督学习模型：半监督支持向量机和标签传播算法。

## S3Vm半监督支持向量机

半监督支持向量机，S3VM(Semi-supervised supporting machine)的原理是，利用有标签的样本去训练一个支持向量机，然后用这个支持向量机去预测未标记的样本，给这部分未标记的样本打上标签，接下来利用这部分有标签加上标记的无标签样本一起训练一个新的向量机，如此重复直到收敛。



![IMG_9800](https://tva1.sinaimg.cn/large/00831rSTgy1gcwr7ulfqnj316i0u0x6p.jpg)



如图所示，+号代表有标签的正样本，-号代表有标签的负样本，绿色的圆圈代表没有标签的样本，原本根据有标签的样本训练出来的超平面就是蓝色虚线所示，在加入无标签的样本以后，重新调整超平面，使得超平面通过数据比较稀疏的区域，调整后的超平面为红色线所示。

### 1. TSVM（Transductive Support machine）

其中S3VM中比较经典的模型就是TSVM，也就是直推式半监督支持向量机。特点是，给样本打标签，然后结合有标签的样本和标记的无标签样本一起训练，得到一个间隔最大化的最优超平面。

具体做法是，TSVM会先根据有标签的样本训练一个SVM，然后用SVM去给没有标记的样本打标签。接下来分别计算每个样本到分类超平面的距离：

$$H_i=wx_ib$$

接下来计算出松弛变量的大小，对于一组数据里面的任意两个标签相反的样本的松弛变量之和大于2，则调转它们的标签，迭代重新训练一个模型，直到最终收敛。

备注：

这里之所以有「任意两个标签相反的样本的松弛变量之和大于2，则调转它们的标签」这一步，这是为了判断出可能打错标签的样本并予以修正。

算法过程：

![image-20200317114522663](https://tva1.sinaimg.cn/large/00831rSTgy1gcws21g1n2j31eh0u07fd.jpg)

## 标签传播算法

LP(Label Propagation)

作用：负样本衍生，作为一种社区发现算法用于识别团伙欺诈。

特点：

1. 基于图

### 原理

这里说明下$l$和$u$分别代表的意义，$l$代表着有标签的样本数量，$u$代表着没有标签的样本数量，一般情况下$l<<u$，也就是无标签的样本数要远高于有标签的样本数。

标签传播算法是基于图而生成的，图上面的每个节点都是一个数据点，数据点与数据点之间通过线连接，称为「关系」，在这里关系就是数据与数据的相似度，越相似，关系越强。边上面有个关键值代表着关系的强弱，就是权重$w_{ij}$代表节点$i$与节点$j$之间的连线的权重。

$w_{ij}$的计算公式为：

$$w_{ij}=exp(-\dfrac{||x_i-x_j||^2}{\sigma})$$

$w_{ij}$的大小与两个数据点之间的特征的距离有关。







标签传播算法的2个关键矩阵：

1. $(l+u)*(l+u)$概率传播矩阵T
2. $(l+u)*C$标签矩阵Y



我们定义一个概率传播矩阵T，其中$T_{ij}$为标签$i$传播到标签$j$的概率，计算公式如下：

$$T_{ij}=P(i\rightarrow j)=\dfrac{w_{ij}}{\sum_{k=1}^{l+u}w_{ik}}$$







![概率传播的示意图](https://tva1.sinaimg.cn/large/00831rSTgy1gcx023e48wj30cw098mxg.jpg)

如上图所示，查看传播图的其中一部分，我们可以看到，其中一个节点为$i$，另外跟它相连的一个节点为$j$，它们之间的权重为$w_{ij}$，而$i$和它周围其他相连的节点也是相同的求法，所有的权重之和作为分母，$w_{ij}$作为分子，可以计算出，节点$i$的标签传播到节点$j$的概率$T_{ij}$，将这个值更新到概率传播矩阵$T$当中的对应位置，如此迭代。

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcx0cmc1rrj30q60jcjtx.jpg)

上面的值都是传播的概率。





根据上面的概率矩阵，我们就可以计算出某一行，也就是某个样本各个类别下的概率分布。

我们再定义一个标签矩阵$(l+u)*C$标签矩阵Y，$C$代表着类别数，比如在下面那个图当中，类别数为3。每个节点在每个类别下的概率等于该标注值乘以对应的权重相加得到，更新标签矩阵Y。



![image-20200317163528452](https://tva1.sinaimg.cn/large/00831rSTgy1gcx0fw1tfaj30ka0g4t9w.jpg)

### 算法描述

input：u个未标记数据和l个标记的数据及其标签
output：u个未标记数据的标签
第一步：初始化，利用权重公式来计算每条边的权重$w_{ij}$，得到数据间的相似度

第二步：根据得到的权重$w_{ij}$计算节点$i$传播到$j$的传播概率，更新到传播概率矩阵$T_{ij}$中

第三步：定义一个(l+u)*C的矩阵，对于未标注的数据里面的数是随机产生的
第四步：每个节点根据传播概率按照权重相加的方式更新到概率分布矩阵当中
第五步：对已标注的数据更新到初始值（也就是1或者0），重复第四步，直到收敛





### 算法实现

在实际操作中，如下图所示，红框中的两个点分别代表着正样本和负样本，橘色的是未标记的样本，通过概率传播算法最终能够把相邻的数据点都打上标签。

![图来自《智能风控》](https://tva1.sinaimg.cn/large/00831rSTgy1gcwyu82exjj31j00u0qv5.jpg)

# 总结

在这里总结了3种方法。

第一种是代价敏感算法，通过给不平衡的样本赋予不同的权重，样本数更少的则相应权重更大，使得模型在训练的时候也会更加关注少量样本的预测表现；

第二种是「插值生成」算法， 本质上是利用了少量的负样本的信息去合成一些不存在的负样本，SMOTE对样本和特征都有一定的要求，要先进行样本和特征清洗，再进行过采样。

第三种是「半监督」方法，通过已有的负样本去推测跟负样本表现接近的其他样本的标签，从而实现扩充数据量的目标。泛化能力会更好，但也可能存在给样本打错标签的可能性，精度不能保证。

基于实际场景，首先使用代价敏感加权的方法，然后尝试两种半监督模型去衍生负样本和正样本。

在实际应用中，可以利用TSVM或者LP算法去给负样本进行打标签，增加负样本的数量，然后再将所有的正样本和负样本放入到有监督模型当中进行训练。

# 参考

1. [一起来读西瓜书：第十三章 半监督学习 - 简书](https://www.jianshu.com/p/7d4323c28716)
2. [标签传播算法(Label Propagation Algorithm)_Python_诗蕊的专栏-CSDN博客](https://blog.csdn.net/Katherine_hsr/article/details/82343647)