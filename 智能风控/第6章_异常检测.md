# 什么是异常检测

异常值检测(Outlier Dectetion)，又称为离群点检测，是找出行为与预期不相符的数据对象。所有的数据集都有一个标准的分布，而异常点就是偏离这个集合分布的点，其特征表现和这个群体很不一样。

正常情况下，异常值点的偏离正常样本的程度是要远大于噪声点的，但是两者并不容易区分。



欺诈检测一般分为个人欺诈和团体欺诈，个人欺诈由于数据量小，且其特征表现与正常的群体很不一样，因此异常值检测适合用于个人欺诈的检测。

而团体欺诈往往是聚集性行为，风控会对聚集性识别为风险，通常使用「基于图的社区发现算法」进行团伙欺诈检测。

# 检测方法

## z-score检验

z检验的前提就是「样本满足正态分布」，原理是计算某个数据点距离总体均值有多少个标准差，来判断对应是异常值点的概率密度。

缺点就是，需要样本集合本身满足正态分布，但是实际上很多数据集本身并不满足正态分布的假设。

## 进行聚类

对数据集进行聚类，异常值点会自动聚成一类，以做到区分正常值和异常值的作用。

## LOF异常检测法

刻画数据密度的方法——局部可达密度，是一种异常度量的方法，相当于给异常值点打分数，其分数越高的话，异常情形越明显。

### 1.算法流程

局部异常算法LOF是将某个样本点p到k近邻的样本点的距离量化成密度的概念，称为局部可达密度。

在该算法中，需要先确定某个样本点周围k个临近点，然后根据公式计算出「局部异常因子」，也就是异常分数。

具体流程是：

1.首先对样本空间的点进行去重复，分别计算每个样本到空间其他点的距离

2.按照升序排列

3.指定近邻样本个数k，对于每个样本点，寻找其k近邻个样本点，然后根据公式计算LOF异常因子。异常因子通过局部可达密度得到。



### 2.原理

几个重要概念：可达距离、局部可达密度、异常因子。

假设说我们要检测的其中一个样本点p是不是异常值，那么我们先找到它k近邻的其中一个样本点o，计算它的可达距离，计算公式是：

$$reach\_dist_k(p, o)=max(k-distance(o),d(p, o))$$

这里的$k-distance(o)$是指样本点o附近第k个最邻近的点的距离，$d(p, o)$则是样本点p和o的直线距离。



这两个距离取最大值就是样本点p到o的可达距离。



局部可达密度则是对样本点p周围所有的样本点都计算一遍可达距离之后，求平均的倒数。

局部可达密度公式是：

$$lrd_k(p)=\dfrac{1}{\dfrac{\sum_{o\in N_k(p)}reach\_dist_k(p,o)}{|N_k(p)|}}$$

> 代表的含义是，样本点p距离k近邻的点越远，表现得越稀疏，那么其密度越小，越有可能是异常值点

但是这样存在一个问题，这里计算是绝对的密度，因为数据本身分布的问题，可能并不是总体均匀的，那么也就存在着可能部分稠密，但相对稀疏的点被当成异常值点。比如下面这种情况。黄色和绿色圈都是一个总体分布下的数据，但是各自有部分聚集的点，其中对于绿色圈里面的某个点p，其可达距离可以从边界的点到黄色区域的点那么长，相应的就会使得计算出的可达密度降低很多，因此也要根据p周围的k近邻样本点的可达距离进行缩放。引入局部因子的概念。

![image-20200316164820102](https://tva1.sinaimg.cn/large/00831rSTgy1gcvv6z0i9mj30tq0sgdj9.jpg)



局部异常因子定义为：

$$LOF_k(p)=\dfrac{\sum_{o\in N_k(p)}\dfrac{lrd_k(o)}{lrd_k(p)}}{|N_k(p)|}=\dfrac{\sum_{o\in N_k(p)}lrd_k(o)}{\dfrac{|N_k(p)|}{lrd_k(p)}}$$

这里的$lrd_k(p)$也就是p点的局部可达密度越小，相应的局部异常因子就会越大。

清洗方案可以尝试用`pyod.models.lof.LOF`

## IF异常检测法

IForest的简单思想就是异常值点距离正常的点更远，显得更稀疏，决策树对某个特征随机选取一个切分节点去分裂，那么异常值点会比正常的点有更大的概率被划分出去到一个单独的叶子节点里。

以下图为例子，如果我们只考察单个维度x的情况，我们随机选取一个分裂的阈值，比如说是$x≥0$和$x<0$，可以将数据分成两部分，接下来继续对数据进行切分，直到数据不可以被切分为止。那么如果B是异常点，A是正常值的话，B要到达叶子节点所需要的切分很明显要少于A，换句话说，B有很大概率切一两刀就能够切分出去。

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcwor7z8hbj30gf0643ye.jpg)

来到二维也是类似的情况。

![IMG_9798](https://tva1.sinaimg.cn/large/00831rSTgy1gcwoqtr9jij31k60u0nf2.jpg)

### 1.算法流程

跟决策树分裂几乎一模一样，不同在于选取特征、样本以及特征的切分点都是随机的。

1. 随机抽取一部分样本，在特征空间随机抽取一个特征，也就是行采样和列采样
2. 在特征上面随机选取一个切分的阈值，按照这个大于或者小于切分阈值分裂为两个节点
3. 重复以上步骤，直到叶子节点不可再分，或者达到预设的分裂深度

### 2.原理

IF异常度分数用到的3个公式：

1）路径长度

$$h(x_i)=e_i+C(T)$$

$h(x_i)$是样本$x_i$的路径长度，$e_i$是样本$x_i$从根节点到最终的叶子节点的边的个数，$C(T)$是偏置项，是指样本$x_i$所在的叶子节点的其他样本的平均路径长度，其中$T$是样本$x_i$所在的叶子节点的其他样本的数量

2）平均路长

偏置项$C(T)$的计算公式如下：

$$C(n)=2H(n-1)-\dfrac{2(n-1)}{n}$$

$n$是当前根节点的样本数，其中$H(k)=ln(k)+\epsilon$，$\epsilon$是欧拉常数，$\epsilon=0.577215$。

3）异常分

$$Score(x_i)=2^{-\dfrac{E(h(x_i))}{C(\phi)}}$$



其中$E(h(x_i))$代表样本$x_i$在所有孤立树上面训练后的路径长度的平均值，$C(\phi)$代表个数为$\phi$的样本在一棵孤立树上面训练后的平均样本长度，起到了归一化的作用。

从上面我们可以看出异常分取值在$[0, 1]$之间，当路径越长的时候，异常分接近于0，越短的时候接近于1，刚好和平均路径长度相同的时候为0.5。

# 应用

LOF异常值检测和IF异常检测法一般在实践当中比较好用，可以在欺诈检测（因为欺诈的用户的特征表现往往跟正常不同），preA模型（利用一些免费的数据在申请评分卡之前就识别出一些负样本降低调用外部征信数据的成本），冷启动模型。

无监督模型的建模难点不在模型身上，而在于需要根据业务知识去挑选出一些具有区分度的特征，因为特征的构造并不能通过对标签的数据分析手段进行。

# 参考

1. [（一）异常检测算法：Isolation Forest原理及其python代码应用_Python_anshuai_aw1的博客-CSDN博客](https://blog.csdn.net/anshuai_aw1/article/details/88425716)
2. 